{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia as wp\n",
    "from wikipedia.exceptions import DisambiguationError, PageError\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('subset', 'JJ'), ('recombination', 'NN'), ('events', 'NNS'), ('results', 'NNS'), ('crossovers', 'NNS'), (',', ','), Tree('REL_PHRASE', [('create', 'VB')]), ('physical', 'JJ'), ('links', 'NNS'), Tree('REL_PHRASE', [('known', 'VBN')]), ('chiasmata', 'NNS'), ('(', '('), ('singular', 'JJ'), (':', ':'), ('chiasma', 'NN'), (',', ','), ('Greek', 'JJ'), ('letter', 'NN'), ('Chi', 'NNP'), ('(', '('), ('X', 'NNP'), (')', ')'), (')', ')'), ('homologous', 'JJ'), ('chromosomes', 'NNS'), ('.', '.')]\n",
      "crossovers\n",
      "(REL_PHRASE create/VB)\n",
      "links\n",
      "(REL_PHRASE known/VBN)\n",
      "chromosomes\n",
      "('.', '.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['crossovers',\n",
       " Tree('REL_PHRASE', [('create', 'VB')]),\n",
       " 'links',\n",
       " Tree('REL_PHRASE', [('known', 'VBN')]),\n",
       " 'chromosomes',\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_ignore = set(stopwords.words('english'))\n",
    "sentence = \"A subset of recombination events results in crossovers, which create physical links known as chiasmata (singular: chiasma, for the Greek letter Chi (X)) between the homologous chromosomes.\"\n",
    "sentence = ' '.join(map(lambda x: x if x not in to_ignore else '', sentence.split(' ')))\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "token_pos = nltk.pos_tag(tokens)\n",
    "verb = \"<VB|VBG|VBN|VBP|VBZ>*<RB|RBR|RBS>*\"\n",
    "word = \"<NN|NNS|NNP|NNPS|JJ|JJR|JJS|RB|WP>\"\n",
    "preposition = \"<IN>\"\n",
    "rel_pattern = \"({}|{}{}|{}{}*{})+ \".format(verb, verb, preposition, verb, word, preposition)\n",
    "grammar_long = '''REL_PHRASE: {%s}''' % rel_pattern\n",
    "reverb_pattern = nltk.RegexpParser(grammar_long)\n",
    "tree = list(reverb_pattern.parse(token_pos))\n",
    "print(tree)\n",
    "\n",
    "def iterate_over(tree):\n",
    "    indices = list(filter(lambda y: y != -1, list(map(lambda x: tree.index(x) if type(x) == nltk.tree.Tree or x[1] == '.' else -1, tree))))\n",
    "    return indices\n",
    "\n",
    "indices = iterate_over(tree)\n",
    "result = []\n",
    "\n",
    "start = 0\n",
    "for i in range(len(indices)):\n",
    "    end = indices[i]\n",
    "    sub_list = list(filter(lambda x: x[1] in ['NN', 'NNS', 'NNP'], tree[start:end]))\n",
    "    last_noun = sub_list[-1][0] if len(sub_list) > 0 else None\n",
    "    print(last_noun)\n",
    "    print(tree[indices[i]])\n",
    "    \n",
    "    if(len(sub_list) > 0):\n",
    "        result.append(last_noun)\n",
    "        result.append(tree[end])\n",
    "    start = end + 1\n",
    "\n",
    "result\n",
    "\n",
    "# iterate over\n",
    "# last NNS/NN if it exists followed by whatever is before REL_PHARSE\n",
    "# REL_PHRASE\n",
    "\n",
    "# note how stage is a common noun that refers to a proper noun\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parent_or_child(topic1, topic2): #returns 1 if topic1 is parent of topic2\n",
    "    try:\n",
    "        topic1_content = wp.page(topic1).content\n",
    "        topic2_content = wp.page(topic2).content\n",
    "        count2in1 = topic1_content.count(topic2) / len(topic1_content)\n",
    "        count1in2 = topic2_content.count(topic1) / len(topic2_content)\n",
    "        if count2in1 == 0 and count1in2 == 0:\n",
    "            return 0\n",
    "        elif count1in2 > count2in1:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    except DisambiguationError or PageError:\n",
    "        return -2\n",
    "\n",
    "# title = \"meiosis\"\n",
    "# page = wp.page(title)\n",
    "# content = ''.join(i for i in page.content if ord(i) < 128)\n",
    "# sents = list(map(lambda x: x.strip(' '), content.split('.')))\n",
    "\n",
    "# for link in page.links:\n",
    "#     comp = parent_or_child(title, link)\n",
    "#     if comp > 0:\n",
    "#         print(title, \"is parent of\", link)\n",
    "#     elif comp < 0:\n",
    "#         print(link, \"is parent of\", title)\n",
    "#     elif comp == 0:\n",
    "#         print(title, link, \"are siblings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_triplet(sent):\n",
    "#     url = \"http://www.newventify.com/rdf?sentence=\"\n",
    "#     r = requests.get(url + sent)\n",
    "#     try:\n",
    "#         rdf = r.json()['rdf']\n",
    "#         print(\"triplet gotten\")\n",
    "#     except JSONDecodeError:\n",
    "#         return []\n",
    "#     return rdf\n",
    "\n",
    "# for sent in sents:\n",
    "#     trip = get_triplet(sent)\n",
    "#     if title in trip:\n",
    "#         print(trip)\n",
    "#         print(sent)\n",
    "\n",
    "# sents = [get_triplet(sent) for sent in sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import to_unicode\n",
    "\n",
    "file_name = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(file_name, binary=True, encoding='utf-8', unicode_errors = 'ignore') \n",
    "# # if you vector file is in binary format, change to binary=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x121c73470>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# def tsne_plot(model):\n",
    "#     \"Creates and TSNE model and plots it\"\n",
    "#     labels = []\n",
    "#     tokens = []\n",
    "\n",
    "#     for word in words:\n",
    "#         if word in model.wv.vocab:\n",
    "#             tokens.append(model[word])\n",
    "#             labels.append(word)\n",
    "    \n",
    "#     tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "#     new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "#     x = []\n",
    "#     y = []\n",
    "#     for value in new_values:\n",
    "#         x.append(value[0])\n",
    "#         y.append(value[1])\n",
    "        \n",
    "#     plt.figure(figsize=(16, 16)) \n",
    "#     for i in range(len(x)):\n",
    "#         plt.scatter(x[i],y[i])\n",
    "#         plt.annotate(labels[i],\n",
    "#                      xy=(x[i], y[i]),\n",
    "#                      xytext=(5, 2),\n",
    "#                      textcoords='offset points',\n",
    "#                      ha='right',\n",
    "#                      va='bottom')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# removes parantheses and spaces\n",
    "def replace_parantheses(pre_string):\n",
    "#     if pre_string.count('(') > pre_string.count(')'):\n",
    "#         first_ind = pre_string.index('(')\n",
    "#         return replace_parantheses(pre_string[:first_ind] + pre_string[first_ind + 1:])\n",
    "    re_match = \"\\(.*\\)\" # anything besides ')'\n",
    "    re_space = \"\\s\\s+\"\n",
    "    return re.sub(re_space, ' ', re.sub(re_match, '', pre_string)).replace('\\n', ' ').replace('\\\"', '')\n",
    "\n",
    "# gets all processed sents of a given topic\n",
    "\n",
    "def get_doc(text):\n",
    "    return sent_tokenize(replace_parantheses(''.join(i for i in text if ord(i) < 128)))\n",
    "\n",
    "def get_all_sents(title):\n",
    "    try:\n",
    "        page = wp.page(title)\n",
    "    except DisambiguationError or PageError:\n",
    "        return []\n",
    "    return get_doc(page.summary)\n",
    "\n",
    "# calls the api which returns an rdf\n",
    "# def get_valid_triplets(sents):\n",
    "#     triplets = [get_triplet(sent) for sent in sents]\n",
    "#     np_triplets = np.array(triplets)\n",
    "#     np_triplets = [np_triplets[i] if (np_triplets[i] != \"\").all() else [] for i in range(len(np_triplets))]\n",
    "#     return np.array(np_triplets).tolist()\n",
    "\n",
    "def get_relevant_terms(title):\n",
    "    try:\n",
    "        page = wp.page(title)\n",
    "    except DisambiguationError or PageError:\n",
    "        return []\n",
    "    return page.links # and those with high cosine scores from our word2vec\n",
    "\n",
    "sents = get_all_sents(\"meiosis\")\n",
    "# sents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS WORKS WELL DIFFERENTIATING BETWEEN HIGHER/LOWER LEVEL AND SIBLING LEVEL\n",
    "# BUT NOT SO WELL BETWEEN PARENT AND CHILD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def get_openie_triplets(sents):\n",
    "    openie_triplets = []\n",
    "    for i in range(len(sents)):\n",
    "        sent = sents[i]\n",
    "        cmd = \"cd Stanford-OpenIE-Python; echo \\“\" + sent + \"\\” > samples.txt; python main.py -f samples.txt\"\n",
    "        trip_array = os.popen(cmd).read().split('\\n')\n",
    "        trip_array = np.array(trip_array)[np.array(trip_array) != ''].tolist()\n",
    "        openie_triplets.append(trip_array)\n",
    "    return openie_triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sent in sents\n",
    "#     get triplet\n",
    "#     get openie_triplets\n",
    "#     get expanded subject, object from openie_triplet\n",
    "#     find highest predicate score of any category of valid openie_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is surprising, very few subject, object pair from triplet is contained entirely in its openie_triplets counterpart\n",
    "# but its clear openie_triplets outperform triplets\n",
    "\n",
    "# we shall weight an idf for openie_triplets predicates in order to calculate score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = [word for sent in sents for word in word_tokenize(sent)]\n",
    "# dct = Dictionary(words)\n",
    "# corpus = [dct.doc2bow(line) for line in words]\n",
    "# import gensim.downloader as api\n",
    "# dataset = api.load(\"text8\")\n",
    "# list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Meiosis between the homologous chromosomes.',\n",
       "  'In most organisms, these links are essential to direct each pair of homologous chromosomes to segregate away from each other during Meiosis I, resulting in two haploid cells that have half the number of chromosomes as the parent cell.',\n",
       "  'During Meiosis II, the cohesion between sister chromatids is released and they segregate from one another, as during mitosis.',\n",
       "  'In some cases all four of the meiotic products form gametes such as sperm, spores, or pollen.',\n",
       "  'In female animals, three of the four meiotic products are typically eliminated by extrusion into polar bodies, and only one cell develops to produce an ovum.',\n",
       "  'Because the number of chromosomes is halved during meiosis, gametes can fuse to form a diploid zygote that contains two copies of each chromosome, one from each parent.',\n",
       "  'Thus, alternating cycles of meiosis and fertilization enable sexual reproduction, with successive generations maintaining the same number of chromosomes.',\n",
       "  'For example, diploid human cells contain 23 pairs of chromosomes including 1 pair of sex chromosomes fuse, the resulting zygote is once again diploid, with the mother and father each contributing 23 chromosomes.',\n",
       "  'This same pattern, but not the same number of chromosomes, occurs in all organisms that utilize meiosis.'],\n",
       " ['Meiosis between the homologous chromosomes.',\n",
       "  'In most organisms, these links are essential to direct each pair of homologous chromosomes to segregate away from each other during Meiosis I, resulting in two haploid cells that have half the number of chromosomes as the parent cell.',\n",
       "  'During Meiosis II, the cohesion between sister chromatids is released and they segregate from one another, as during mitosis.',\n",
       "  'In some cases all four of the meiotic products form gametes such as sperm, spores, or pollen.',\n",
       "  'In female animals, three of the four meiotic products are typically eliminated by extrusion into polar bodies, and only one cell develops to produce an ovum.',\n",
       "  'Because the number of chromosomes is halved during meiosis, gametes can fuse to form a diploid zygote that contains two copies of each chromosome, one from each parent.',\n",
       "  'Thus, alternating cycles of meiosis and fertilization enable sexual reproduction, with successive generations maintaining the same number of chromosomes.',\n",
       "  'For example, diploid human cells contain 23 pairs of chromosomes including 1 pair of sex chromosomes fuse, the resulting zygote is once again diploid, with the mother and father each contributing 23 chromosomes.',\n",
       "  'This same pattern, but not the same number of chromosomes, occurs in all organisms that utilize meiosis.'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll create a general tf-idf model for domain topics in related topics, then be cached later\n",
    "# specifically, the hyperlinks in the summary for the topic will be added each time it's called\n",
    "\n",
    "# gets all docs with each being a link of meiosis (only need to do this once)\n",
    "def get_docs(topic):\n",
    "    docs = []\n",
    "    try:\n",
    "        pg = wp.page(topic)\n",
    "    except DisambiguationError or PageError:\n",
    "        return []\n",
    "    first_doc = get_doc(pg.summary)\n",
    "    all_docs = get_doc(pg.content)\n",
    "    docs.append(first_doc)\n",
    "    for i in range(1, (len(all_docs) + len(first_doc)) // len(first_doc)):\n",
    "        docs.append(all_docs[len(first_doc) * i:len(first_doc) * (i + 1)])\n",
    "    return [doc for doc in docs if len(doc) > 0]\n",
    "\n",
    "# docs = get_docs(\"meiosis\")\n",
    "\n",
    "get_docs(\"meiosis\")[0], get_doc(wp.page(\"meiosis\").summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs is in the form of [[sent, sent, ...], [sent, sent, ...], ...] -> [[word, word, ...], [word, word, ...], ...]\n",
    "def docs_to_vocabs(docs):\n",
    "    passages = [' '.join(doc).lower() for doc in docs]\n",
    "    words = [list(map(lambda x: x.strip('.').strip(','), passage.split())) for passage in passages]\n",
    "    return words\n",
    "    \n",
    "# assumes docs is in the form of [[sent, sent, ...], [sent, sent, ...], ...]\n",
    "# returns tfidf model\n",
    "def train_tfidf(docs):\n",
    "    passages_to_vocabs = docs_to_vocabs(docs)\n",
    "    dct = Dictionary(passages_to_vocabs)\n",
    "    corpus = [dct.doc2bow(passage) for passage in passages_to_vocabs]\n",
    "    tfidf_model = TfidfModel(corpus)\n",
    "    return tfidf_model, corpus, dct\n",
    "    \n",
    "# tfidf_model[dct.doc2bow(\"with each other\".split())]\n",
    "# token_dct = {key.lower(): dct.token2id[key] for key in dct.token2id.keys()}\n",
    "\n",
    "# takes the doc for the topic and extracts weighed by the tf-idf's of phrase\n",
    "\n",
    "def phrase_to_vec(phrase, dct, vocab_scores):\n",
    "    scores = []\n",
    "    phrase_arr = phrase.split()\n",
    "    for word in phrase_arr:\n",
    "        if word not in model.wv.vocab:\n",
    "            phrase_arr.remove(word)\n",
    "            continue\n",
    "        if dct.token2id[word] in vocab_scores.keys():\n",
    "            scores.append(vocab_scores[dct.token2id[word]])\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    weights = np.array(scores) / sum(scores) # to standardize and preserve proportions\n",
    "    vec = sum([weights[i] * model.get_vector(phrase.split()[i]) for i in range(len(weights))])\n",
    "    \n",
    "    # to make sure it's a unit vector\n",
    "    return np.array(vec) / np.linalg.norm(vec)\n",
    "\n",
    "# series of sanity checks\n",
    "# sanity_vocabs = docs_to_vocabs(docs[:1])[0]\n",
    "# phrase_to_vec(\"once again\")\n",
    "# print(sanity_vocabs.count(\"once\"), sanity_vocabs.count(\"again\"))\n",
    "# phrase_to_vec(\"with each other\")\n",
    "# print(sanity_vocabs.count(\"with\"), sanity_vocabs.count(\"each\"), sanity_vocabs.count(\"other\"))\n",
    "# phrase_to_vec(\"do not have\")\n",
    "# print(sanity_vocabs.count(\"do\"), sanity_vocabs.count(\"not\"), sanity_vocabs.count(\"have\"))\n",
    "\n",
    "# dct.token2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_words = [\"includes\", \"contains\"]\n",
    "sibling_words = [\"causes\", \"caused\"]\n",
    "child_words = [\"suchas\"]\n",
    "\n",
    "# uses phrase_to_vec\n",
    "def similarity(pred, word, dct, vocab_scores):\n",
    "    return sum(phrase_to_vec(pred, dct, vocab_scores) * model.get_vector(word)) / (np.linalg.norm(phrase_to_vec(pred, dct, vocab_scores)) * np.linalg.norm(model.get_vector(word)))\n",
    "\n",
    "def max_sim(relation, pred, dct, vocab_scores):\n",
    "    if relation in [\"parent\", \"sibling\", \"child\"]:\n",
    "        return sum([similarity(pred, word, dct, vocab_scores) for word in globals()[\"{}_words\".format(relation)]]) / len(globals()[\"{}_words\".format(relation)])\n",
    "\n",
    "def best_relation(pred, dct, vocab_scores):\n",
    "    # if more than one word in pred is not in model vocab, return unknown, as otherwise is too confounding\n",
    "    if len(list(filter(lambda x: x.lower() in model.wv.vocab, pred.split()))) < len(pred.split()) - 1:\n",
    "        return \"unknown\"\n",
    "    best_rel = max([\"parent\", \"sibling\", \"child\"], key = lambda x: max_sim(x, pred, dct, vocab_scores))\n",
    "    return best_rel, max_sim(best_rel, pred, dct, vocab_scores)\n",
    "\n",
    "def subject_relation(triplet):\n",
    "    print(triplet)\n",
    "    return best_relation(triplet[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthesizing everything so far\n",
    "\n",
    "def get_rels(sents, topic):\n",
    "    relationships = []\n",
    "    docs = get_docs(topic)\n",
    "    tfidf_model, corpus, dct = train_tfidf(docs)\n",
    "    # first_doc_dct = dct.doc2bow(docs_to_vocabs(docs[:1])[0])\n",
    "    vocab_scores = {vocab[0]: vocab[1] for vocab in tfidf_model[corpus[0]]}\n",
    "    for i in range((len(sents) + 10) // 10):\n",
    "        batch = sents[10 * i: min(len(sents), 10 * (i + 1))]\n",
    "        triplets = get_openie_triplets(batch)\n",
    "        print(\"triplets length\", len(triplets))\n",
    "        # given an openie_triplet\n",
    "        # iterates looking for valid subject / object pair, will alias subject / pred / object as spo_list\n",
    "        # for valid pairs, computes phrase_to_vec for a parent/child/sibling score\n",
    "        # takes most decisive of these scores\n",
    "        for triplet in triplets:\n",
    "            spo_list = list(map(lambda x: list(map(lambda y: y.strip(), x.split('|'))), triplet))\n",
    "            spo_list = list(filter(lambda x: len(x) == 3, spo_list))\n",
    "            for i in range(len(spo_list)):\n",
    "                if len(spo_list[i]) == 3:\n",
    "                    spo_list[i][1] = best_relation(spo_list[i][1].lower(), dct, vocab_scores)\n",
    "            relationships.append(spo_list)    \n",
    "    return relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove those for which best_rel score is nan\n",
    "def filtr(relationships):\n",
    "    # filters one with nan as best relationship score\n",
    "    best_rels = relationships\n",
    "    for i in range(len(best_rels)):\n",
    "        best_rels[i] = list(filter(lambda x: not np.isnan(x[1][1]), best_rels[i]))\n",
    "    # gets best relation\n",
    "    for i in range(len(best_rels)):\n",
    "        best_rels[i] = max(best_rels[i], key = lambda x: x[1][1]) if len(best_rels[i]) > 0 else []\n",
    "    assert len(best_rels) == len(relationships)\n",
    "    return best_rels\n",
    "        \n",
    "# non_null_relships = list(filter(lambda x: len(x) > 0, relationships))\n",
    "# for i in range(len(non_null_relships)):\n",
    "#     if len(non_null_relships[i]) > 0:\n",
    "#         non_null_relships[i] = list(filter(lambda x: nltk.pos_tag(x[0])[1] in ['NN', 'NNS', 'NNP'] and nltk.pos_tag(x[2])[1] in ['NN', 'NNS', 'NNP'], non_null_relships[i]))\n",
    "# non_null_relships = list(filter(lambda x: len(x) > 0, non_null_relships))\n",
    "# non_null_relships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crude but will use this to extract noun phrases and replace with single nouns\n",
    "# i.e. \"number\" in best_rels[5] will become number of chromosomes\n",
    "NP = \"NP: {<JJ>?<NN|NNS>(<IN><NN|NNS>)?}\"\n",
    "chunker = nltk.RegexpParser(NP)\n",
    "\n",
    "def replace_with_chunker(best_rels, sents):\n",
    "    print(\"start of chunker\", best_rels, sents, \"is sents\")\n",
    "    for i in range(len(sents)):\n",
    "        if len(best_rels[i]) > 0:\n",
    "            parser_chunks = list(chunker.parse(nltk.pos_tag(word_tokenize(sents[5]))))\n",
    "            parser_chunks = list(filter(lambda x: type(x) == nltk.tree.Tree, parser_chunks))\n",
    "            replace_chunks = [' '.join(list(map(lambda x: x[0], list(parser_chunks[i])))) for i in range(len(parser_chunks))]\n",
    "            parser_chunks = [replace_chunk.split() for replace_chunk in replace_chunks]\n",
    "            for chunk in replace_chunks:\n",
    "                if chunk.find(best_rels[i][0]) > -1:\n",
    "                    best_rels[i][0] = chunk\n",
    "                    break\n",
    "            for chunk in reversed(replace_chunks):\n",
    "                if chunk.find(best_rels[i][2]) > -1:\n",
    "                    best_rels[i][2] = chunk\n",
    "                    break\n",
    "        \n",
    "    return best_rels\n",
    "\n",
    "# here we convert everything to a key: [(match, relation, score)] \n",
    "# parent, child, sibling relation => (1, -1, 0)\n",
    "def store_into_dict(best_rels):\n",
    "    rels_dic = {}\n",
    "    rel_dic = {'parent': 1, 'child': -1, 'sibling': 0}\n",
    "    for rel in best_rels:\n",
    "        if len(rel) == 0:\n",
    "            continue\n",
    "        if rel[0] in rels_dic.keys():\n",
    "            rels_dic[rel[0]].append([rel[2], rel_dic[rel[1][0]], rel[1][1]])\n",
    "        else:\n",
    "            rels_dic[rel[0]] = [[rel[2], rel_dic[rel[1][0]], rel[1][1]]]\n",
    "    return rels_dic\n",
    "\n",
    "# this didn't work as well as hoped, with ambiguous terms\n",
    "# the big takeaway is that getting triplets from the entire page content is very noisy\n",
    "# instead, focus on summaries and recursively extract children that way\n",
    "# with adjustable feedback loops to adapt to the depth parameter and avoid opening up too many pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a recursive function that, given a topic, gets all relations via its page summary\n",
    "def collect_relatives(topic):\n",
    "    print(\"collecting relatives of\", topic)\n",
    "    sents = get_all_sents(topic)\n",
    "    if len(sents) == 0:\n",
    "        return 0\n",
    "    relationships = get_rels(sents, topic)\n",
    "    best_rels = replace_with_chunker(filtr(relationships), sents)\n",
    "    rels_dict = store_into_dict(best_rels)\n",
    "    return rels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur tree meiosis with 4 levels to fill\n",
      "cur tree {'meiosis': {}} with 4 levels to fill\n",
      "collecting relatives of meiosis\n",
      "0 of 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-c5e4f4722d67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mbuilt_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meiosis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-c5e4f4722d67>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(tree, rel_scores, max_depth, depth)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# if tree has no value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-c5e4f4722d67>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(tree, rel_scores, max_depth, depth)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# if tree has no value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mrels_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_relatives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrels_dict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mchildren_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrels_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrels_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-04191d54d401>\u001b[0m in \u001b[0;36mcollect_relatives\u001b[0;34m(topic)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"collecting relatives of\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mrelationships\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mbest_rels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace_with_chunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelationships\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrels_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_into_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_rels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-b171355f91af>\u001b[0m in \u001b[0;36mget_rels\u001b[0;34m(sents, topic)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"of\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtriplets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_openie_triplets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"triplets length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriplets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# given an openie_triplet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-58d492b76387>\u001b[0m in \u001b[0;36mget_openie_triplets\u001b[0;34m(sents)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cd Stanford-OpenIE-Python; echo \\“\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\” > samples.txt; python main.py -f samples.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtrip_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtrip_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrip_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrip_array\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mopenie_triplets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrip_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tree should look like {topic: {sub_topic: None}}\n",
    "\n",
    "# given a tree, goes to all layers depth deep and generates children of it, until depth hits max_depth\n",
    "def build_tree(tree, rel_scores, max_depth = 4, depth = 0):\n",
    "    print(\"cur tree\", tree, \"with\", max_depth + 1 - depth, \"levels to fill\")\n",
    "    if depth > max_depth:\n",
    "        return tree\n",
    "    if type(tree) == str:\n",
    "        return build_tree({tree: {}}, rel_scores, max_depth, depth)\n",
    "    # if tree has no value\n",
    "    if not len(list(tree[list(tree.keys())[0]].keys())):   \n",
    "        rels_dict = collect_relatives(list(tree.keys())[0])\n",
    "        if type(rels_dict) != int:\n",
    "            children_keys = list(filter(lambda x: rels_dict[x][0][1] == 1, rels_dict.keys()))\n",
    "            # for later reference, merges rels_dict with a cached rel_scores\n",
    "            rel_scores.update({key: rels_dict[key] for key in rels_dict.keys()})\n",
    "            # also merges the reversed rels_dict for easier cached reference\n",
    "            rel_scores.update({rels_dict[key][0][0]: [key, -rels_dict[key][0][1], rels_dict[key][0][2]] for key in rels_dict.keys()})\n",
    "            tree[list(tree.keys())[0]] = {key: {rels_dict[key][0][0]: {}} for key in children_keys}      \n",
    "    else:\n",
    "        cur_key = list(tree.keys())[0]\n",
    "        for key in tree[cur_key].keys():\n",
    "            tree.update(build_tree(tree[cur_key][key], rel_scores, max_depth, depth + 1))\n",
    "    return build_tree(tree, rel_scores, max_depth, depth + 1)\n",
    "    \n",
    "built_tree = build_tree(\"meiosis\", {}, max_depth = 3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "exceptions must derive from BaseException",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDisambiguationError\u001b[0m        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-c616f3d821dc>\u001b[0m in \u001b[0;36mget_all_sents\u001b[0;34m(title)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mDisambiguationError\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mPageError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/106XFinal/WikiScraper/scrape_venv/lib/python3.6/site-packages/wikipedia/wikipedia.py\u001b[0m in \u001b[0;36mpage\u001b[0;34m(title, pageid, auto_suggest, redirect, preload)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mpageid\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/106XFinal/WikiScraper/scrape_venv/lib/python3.6/site-packages/wikipedia/wikipedia.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, title, pageid, redirect, preload, original_title)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/106XFinal/WikiScraper/scrape_venv/lib/python3.6/site-packages/wikipedia/wikipedia.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self, redirect, preload)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mDisambiguationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmay_refer_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDisambiguationError\u001b[0m: \"Released\" may refer to: \nReleased (Jade Warrior album)\nReleased (Patti LaBelle album)\nReleased (Westlife album)\nDreams of Endless War\nReleased: 1985–1995\nRelease (disambiguation)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-4da43d2f6edf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_all_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"released\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-85-c616f3d821dc>\u001b[0m in \u001b[0;36mget_all_sents\u001b[0;34m(title)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mDisambiguationError\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mPageError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0;34m\"No title\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: exceptions must derive from BaseException"
     ]
    }
   ],
   "source": [
    "get_all_sents(\"released\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rel_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-040f0ed82a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrel_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rel_scores' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape_venv",
   "language": "python",
   "name": "scrape_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
